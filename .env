# Hugging Face Hub Configuration
HF_HUB_TOKEN_WRITE="your_huggingface_write_token_here"
HF_HUB_TOKEN_READ="your_huggingface_read_token_here" # Can be same as write token if it has read access
HF_MODEL_ID="your-username/your-ko-en-translator-model-name" # e.g., MyName/ko-en-opus-mt-qlora-ft
BASE_MODEL_ID="Helsinki-NLP/opus-mt-ko-en"
DATASET_ID="klei22/korean-english-jamon-parallel-corpora"
DATASET_FILE="ko_ja_en.parquet" # This is the file within the dataset repo

# Papago API Configuration
PAPAGO_CLIENT_ID="your_papago_client_id"
PAPAGO_CLIENT_SECRET="your_papago_client_secret"

# Training Configuration (Optional, can be hardcoded or moved to a YAML config too)
NUM_TRAIN_EPOCHS=3
LEARNING_RATE=3e-4
PER_DEVICE_TRAIN_BATCH_SIZE=16 # Adjust based on your GPU memory
PER_DEVICE_EVAL_BATCH_SIZE=16  # Adjust based on your GPU memory
MAX_SAMPLES_DATASET=1000 # For quick testing, set to None for full dataset
OUTPUT_DIR="./results"
QLORA_R=8
QLORA_ALPHA=32
QLORA_DROPOUT=0.05